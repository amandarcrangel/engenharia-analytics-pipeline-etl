# engenharia-analytics-pipeline-etl

# Storytelling: Treinamento para Engenheiro(a) de Analytics Júnior na Zoop 🚀

## 📚 Contexto
Imagine que você está participando de um treinamento exclusivo para o cargo de **Engenheiro(a) de Analytics Júnior** na Zoop. Durante essa jornada, o **Head de Dados** da empresa compartilha com você **três bases de dados**. Sua missão? Utilizar serviços da AWS para analisá-los e criar **relatórios e dashboards** que transformem dados em decisões.

---

## 🛠️ O Desafio
O treinamento foca em dois elementos fundamentais da Engenharia de Dados: **ingestão de dados** e **processo ETL (Extração, Transformação e Carga)**. Sua tarefa será criar as camadas principais do projeto:

### 🔹 Camada Bronze  
Armazena os **dados brutos**, exatamente como foram recebidos.

### 🔹 Camada Silver  
Contém os **dados tratados**, ajustados conforme as diretrizes do Head de Dados.

Para facilitar, você terá acesso aos serviços AWS por meio de um **usuário IAM**, configurado especialmente para o treinamento.

---

## 💾 Bases de Dados
Você trabalhará com as seguintes bases, todas armazenadas em formato Parquet:

- **vendas_zoop_bronze.parquet**  
- **estoques_zoop_bronze.parquet**  
- **redes_sociais_zoop_bronze.parquet**

---

## 🎯 Objetivo do Treinamento
Construir um pipeline de ETL robusto utilizando o **AWS Glue**. O processo inclui:

1. **Ingestão de dados** em um bucket S3.  
2. Criação das **camadas bronze** (dados brutos) e **silver** (dados transformados).  
3. Configuração de banco de dados e tabelas no **AWS Glue Data Catalog**.  
4. Desenvolvimento de pipelines de ETL no **AWS Glue Studio**.  
5. Realização de consultas ad-hoc no **AWS Athena**.  
6. Geração de relatórios interativos no **AWS QuickSight**.

---

## 🔧 Tech Stack
Prepare-se para dominar as seguintes tecnologias:

- **Amazon S3**  
- **AWS Glue Crawler**  
- **AWS Glue Data Catalog**  
- **AWS Glue ETL Job (Studio)**  
- **AWS Glue Data Quality**  
- **Python**  
- **Apache Spark**  
- **Athena**  
- **SQL**

---

## 💡 O Que eu aprendi
- Construir pipelines de ETL.  
- Organizar dados para consultas eficientes.  
- Criar visualizações impactantes.

---


## 📋 Pré-requisitos
Você precisará de conhecimentos básicos de:

- **Python**, especialmente com a biblioteca **PySpark** para manipular grandes volumes de dados utilizando **DataFrames** e comandos SQL.

---

## ⚠️ Alerta Importante
Trabalhar com AWS envolve custos. Durante alguns passado do treinamento, não foi realizado apenas observado com foi feito. 

---

