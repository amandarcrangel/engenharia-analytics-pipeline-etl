# engenharia-analytics-pipeline-etl

# Storytelling: Treinamento para Engenheiro(a) de Analytics JÃºnior na Zoop ğŸš€

## ğŸ“š Contexto
Imagine que vocÃª estÃ¡ participando de um treinamento exclusivo para o cargo de **Engenheiro(a) de Analytics JÃºnior** na Zoop. Durante essa jornada, o **Head de Dados** da empresa compartilha com vocÃª **trÃªs bases de dados**. Sua missÃ£o? Utilizar serviÃ§os da AWS para analisÃ¡-los e criar **relatÃ³rios e dashboards** que transformem dados em decisÃµes.

---

## ğŸ› ï¸ O Desafio
O treinamento foca em dois elementos fundamentais da Engenharia de Dados: **ingestÃ£o de dados** e **processo ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga)**. Sua tarefa serÃ¡ criar as camadas principais do projeto:

### ğŸ”¹ Camada Bronze  
Armazena os **dados brutos**, exatamente como foram recebidos.

### ğŸ”¹ Camada Silver  
ContÃ©m os **dados tratados**, ajustados conforme as diretrizes do Head de Dados.

Para facilitar, vocÃª terÃ¡ acesso aos serviÃ§os AWS por meio de um **usuÃ¡rio IAM**, configurado especialmente para o treinamento.

---

## ğŸ’¾ Bases de Dados
VocÃª trabalharÃ¡ com as seguintes bases, todas armazenadas em formato Parquet:

- **vendas_zoop_bronze.parquet**  
- **estoques_zoop_bronze.parquet**  
- **redes_sociais_zoop_bronze.parquet**

---

## ğŸ¯ Objetivo do Treinamento
Construir um pipeline de ETL robusto utilizando o **AWS Glue**. O processo inclui:

1. **IngestÃ£o de dados** em um bucket S3.  
2. CriaÃ§Ã£o das **camadas bronze** (dados brutos) e **silver** (dados transformados).  
3. ConfiguraÃ§Ã£o de banco de dados e tabelas no **AWS Glue Data Catalog**.  
4. Desenvolvimento de pipelines de ETL no **AWS Glue Studio**.  
5. RealizaÃ§Ã£o de consultas ad-hoc no **AWS Athena**.  
6. GeraÃ§Ã£o de relatÃ³rios interativos no **AWS QuickSight**.

---

## ğŸ”§ Tech Stack
Prepare-se para dominar as seguintes tecnologias:

- **Amazon S3**  
- **AWS Glue Crawler**  
- **AWS Glue Data Catalog**  
- **AWS Glue ETL Job (Studio)**  
- **AWS Glue Data Quality**  
- **Python**  
- **Apache Spark**  
- **Athena**  
- **SQL**

---

## ğŸ’¡ O Que eu aprendi
- Construir pipelines de ETL.  
- Organizar dados para consultas eficientes.  
- Criar visualizaÃ§Ãµes impactantes.

---


## ğŸ“‹ PrÃ©-requisitos
VocÃª precisarÃ¡ de conhecimentos bÃ¡sicos de:

- **Python**, especialmente com a biblioteca **PySpark** para manipular grandes volumes de dados utilizando **DataFrames** e comandos SQL.

---

## âš ï¸ Alerta Importante
Trabalhar com AWS envolve custos. Durante alguns passado do treinamento, nÃ£o foi realizado apenas observado com foi feito. 

---

